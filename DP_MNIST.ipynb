{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrk0aWZe2B-C",
        "outputId": "d85f4bd9-88e7-4e06-d67a-6a57e2a63fb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 - Loss: 1.2169, Test Accuracy: 86.58%\n",
            "Epoch 02 - Loss: 0.4888, Test Accuracy: 89.27%\n",
            "Epoch 03 - Loss: 0.3923, Test Accuracy: 89.92%\n",
            "Epoch 04 - Loss: 0.3530, Test Accuracy: 90.99%\n",
            "Epoch 05 - Loss: 0.3298, Test Accuracy: 91.04%\n",
            "Epoch 06 - Loss: 0.3148, Test Accuracy: 91.32%\n",
            "Epoch 07 - Loss: 0.3022, Test Accuracy: 91.56%\n",
            "Epoch 08 - Loss: 0.2927, Test Accuracy: 92.09%\n",
            "Epoch 09 - Loss: 0.2833, Test Accuracy: 91.98%\n",
            "Epoch 10 - Loss: 0.2751, Test Accuracy: 92.05%\n",
            "Baseline DP-SGD finished. (Privacy accounting was not detailed here.)\n"
          ]
        }
      ],
      "source": [
        "# dp_mnist.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Hyperparameters for DP (fixed parameters)\n",
        "fixed_clip_norm = 1.0         # fixed clipping threshold\n",
        "noise_multiplier = 0.1        # fixed noise multiplier (σ)\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "learning_rate = 0.01\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define a simple neural network for MNIST\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Prepare the MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True,\n",
        "                               transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False,\n",
        "                              transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += (pred == target).sum().item()\n",
        "            total += target.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# Training loop implementing DP-SGD with fixed parameters\n",
        "def train_dp():\n",
        "    model = SimpleMLP().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(data)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # Compute total gradient norm (as a proxy for per-sample norms)\n",
        "            total_norm = 0.0\n",
        "            for param in model.parameters():\n",
        "                if param.grad is not None:\n",
        "                    param_norm = param.grad.data.norm(2)\n",
        "                    total_norm += param_norm.item() ** 2\n",
        "            total_norm = total_norm ** 0.5\n",
        "\n",
        "            # Clip gradients if needed\n",
        "            clip_coef = fixed_clip_norm / (total_norm + 1e-6)\n",
        "            if clip_coef < 1:\n",
        "                for param in model.parameters():\n",
        "                    if param.grad is not None:\n",
        "                        param.grad.data.mul_(clip_coef)\n",
        "\n",
        "            # Add Gaussian noise to each gradient\n",
        "            for param in model.parameters():\n",
        "                if param.grad is not None:\n",
        "                    noise = torch.normal(mean=0, std=noise_multiplier * fixed_clip_norm, size=param.grad.data.shape, device=device)\n",
        "                    param.grad.data.add_(noise)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        test_acc = evaluate(model, test_loader, device)\n",
        "        print(f'Epoch {epoch:02d} - Loss: {avg_loss:.4f}, Test Accuracy: {test_acc*100:.2f}%')\n",
        "\n",
        "    # (For demonstration, we print a fixed privacy budget message)\n",
        "    print(\"Baseline DP-SGD finished. (Privacy accounting was not detailed here.)\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_dp()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adp_mnist_fixed.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Initial hyperparameters for adaptive DP\n",
        "init_clip_norm = 1.0          # starting clipping threshold\n",
        "init_noise_multiplier = 0.1   # initial noise multiplier (σ)\n",
        "alpha = 0.2                   # factor to scale the average gradient norm (must be <= 1 to dampen explosive growth)\n",
        "tau = 0.1                     # smoothing factor for moving average update of clip_norm\n",
        "beta = 0.9                    # decay factor for noise multiplier when validation improves\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "learning_rate = 0.01\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define a simple neural network for MNIST\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Prepare the MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True,\n",
        "                               transform=transform, download=True)\n",
        "# Use part of the training set as a \"validation\" set\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=1000, shuffle=False)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False,\n",
        "                              transform=transform, download=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Evaluation function (for both validation and test)\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    loss_total = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            loss_total += F.cross_entropy(outputs, target, reduction='sum').item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == target).sum().item()\n",
        "            total += target.size(0)\n",
        "    return loss_total / total, correct / total\n",
        "\n",
        "# Training loop implementing adaptive DP-SGD with stability fixes\n",
        "def train_adp():\n",
        "    model = SimpleMLP().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    clip_norm = init_clip_norm\n",
        "    noise_multiplier = init_noise_multiplier\n",
        "    val_loss_history = []  # record validation losses to check for consecutive decreases\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        batch_grad_norms = []  # record gradient norms in this epoch\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(data)\n",
        "            loss = F.cross_entropy(outputs, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # Compute global gradient norm (all parameters combined)\n",
        "            total_norm_sq = 0.0\n",
        "            for param in model.parameters():\n",
        "                if param.grad is not None:\n",
        "                    total_norm_sq += param.grad.data.norm(2).item() ** 2\n",
        "            total_norm = total_norm_sq ** 0.5\n",
        "            batch_grad_norms.append(total_norm)\n",
        "\n",
        "            # Clip gradients using the current adaptive clip_norm\n",
        "            clip_coef = clip_norm / (total_norm + 1e-6)\n",
        "            if clip_coef < 1:\n",
        "                for param in model.parameters():\n",
        "                    if param.grad is not None:\n",
        "                        param.grad.data.mul_(clip_coef)\n",
        "\n",
        "            # Add Gaussian noise using the current clip_norm and noise multiplier\n",
        "            for param in model.parameters():\n",
        "                if param.grad is not None:\n",
        "                    noise = torch.normal(mean=0, std=noise_multiplier * clip_norm, size=param.grad.data.shape, device=device)\n",
        "                    param.grad.data.add_(noise)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        avg_grad_norm = sum(batch_grad_norms) / len(batch_grad_norms)\n",
        "\n",
        "        # Adaptive update: update clip_norm using a moving average to smooth the changes.\n",
        "        measured_clip = alpha * avg_grad_norm\n",
        "        new_clip_norm = (1 - tau) * clip_norm + tau * measured_clip\n",
        "\n",
        "        # Evaluate on the validation set\n",
        "        val_loss, val_acc = evaluate(model, val_loader, device)\n",
        "        val_loss_history.append(val_loss)\n",
        "\n",
        "        # Decay noise multiplier if validation loss decreases for three consecutive epochs\n",
        "        if len(val_loss_history) >= 3 and val_loss_history[-3] > val_loss_history[-2] > val_loss_history[-1]:\n",
        "            noise_multiplier = beta * noise_multiplier\n",
        "            print(\"Validation loss decreased three epochs in a row. Reducing noise multiplier.\")\n",
        "\n",
        "        # Update clip_norm for the next epoch\n",
        "        clip_norm = new_clip_norm\n",
        "\n",
        "        # Evaluate on the test set for monitoring\n",
        "        test_loss, test_acc = evaluate(model, test_loader, device)\n",
        "        print(f\"Epoch {epoch:02d}: Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Test Acc: {test_acc*100:.2f}%\")\n",
        "        print(f\"  (Adaptive clip norm: {clip_norm:.4f}, noise multiplier: {noise_multiplier:.4f})\")\n",
        "\n",
        "    print(\"Adaptive DP-SGD finished. (Privacy accounting was simulated.)\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_adp()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnaqQfFO22gR",
        "outputId": "222a1dcd-c14e-4a2b-96d5-a0f144ab7c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01: Train Loss: 1.3210, Val Loss: 0.6808, Test Acc: 85.03%\n",
            "  (Adaptive clip norm: 0.9184, noise multiplier: 0.1000)\n",
            "Epoch 02: Train Loss: 0.5443, Val Loss: 0.4636, Test Acc: 88.60%\n",
            "  (Adaptive clip norm: 0.8446, noise multiplier: 0.1000)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 03: Train Loss: 0.4272, Val Loss: 0.3985, Test Acc: 89.48%\n",
            "  (Adaptive clip norm: 0.7783, noise multiplier: 0.0900)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 04: Train Loss: 0.3801, Val Loss: 0.3641, Test Acc: 90.25%\n",
            "  (Adaptive clip norm: 0.7187, noise multiplier: 0.0810)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 05: Train Loss: 0.3540, Val Loss: 0.3465, Test Acc: 90.85%\n",
            "  (Adaptive clip norm: 0.6655, noise multiplier: 0.0729)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 06: Train Loss: 0.3369, Val Loss: 0.3298, Test Acc: 91.14%\n",
            "  (Adaptive clip norm: 0.6174, noise multiplier: 0.0656)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 07: Train Loss: 0.3230, Val Loss: 0.3186, Test Acc: 91.39%\n",
            "  (Adaptive clip norm: 0.5743, noise multiplier: 0.0590)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 08: Train Loss: 0.3117, Val Loss: 0.3080, Test Acc: 91.60%\n",
            "  (Adaptive clip norm: 0.5354, noise multiplier: 0.0531)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 09: Train Loss: 0.3016, Val Loss: 0.2997, Test Acc: 91.79%\n",
            "  (Adaptive clip norm: 0.5002, noise multiplier: 0.0478)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 10: Train Loss: 0.2935, Val Loss: 0.2922, Test Acc: 92.05%\n",
            "  (Adaptive clip norm: 0.4684, noise multiplier: 0.0430)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 11: Train Loss: 0.2867, Val Loss: 0.2867, Test Acc: 92.25%\n",
            "  (Adaptive clip norm: 0.4397, noise multiplier: 0.0387)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 12: Train Loss: 0.2804, Val Loss: 0.2815, Test Acc: 92.33%\n",
            "  (Adaptive clip norm: 0.4138, noise multiplier: 0.0349)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 13: Train Loss: 0.2748, Val Loss: 0.2773, Test Acc: 92.57%\n",
            "  (Adaptive clip norm: 0.3905, noise multiplier: 0.0314)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 14: Train Loss: 0.2701, Val Loss: 0.2735, Test Acc: 92.64%\n",
            "  (Adaptive clip norm: 0.3694, noise multiplier: 0.0282)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 15: Train Loss: 0.2657, Val Loss: 0.2700, Test Acc: 92.77%\n",
            "  (Adaptive clip norm: 0.3504, noise multiplier: 0.0254)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 16: Train Loss: 0.2615, Val Loss: 0.2659, Test Acc: 92.76%\n",
            "  (Adaptive clip norm: 0.3329, noise multiplier: 0.0229)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 17: Train Loss: 0.2574, Val Loss: 0.2622, Test Acc: 92.91%\n",
            "  (Adaptive clip norm: 0.3173, noise multiplier: 0.0206)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 18: Train Loss: 0.2539, Val Loss: 0.2588, Test Acc: 93.03%\n",
            "  (Adaptive clip norm: 0.3030, noise multiplier: 0.0185)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 19: Train Loss: 0.2506, Val Loss: 0.2562, Test Acc: 93.06%\n",
            "  (Adaptive clip norm: 0.2903, noise multiplier: 0.0167)\n",
            "Validation loss decreased three epochs in a row. Reducing noise multiplier.\n",
            "Epoch 20: Train Loss: 0.2475, Val Loss: 0.2539, Test Acc: 93.12%\n",
            "  (Adaptive clip norm: 0.2785, noise multiplier: 0.0150)\n",
            "Adaptive DP-SGD finished. (Privacy accounting was simulated.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Split‐Learning DP MNIST Training Script\n",
        "- Client‐side and Server‐side architectures are defined by user.\n",
        "- Noise added only on the server side gradients (fixed or adaptive DP).\n",
        "- Logs all experiment parameters and outcomes to a JSONL log file.\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def parse_layer_sizes(prompt):\n",
        "    \"\"\"Helper to parse comma‐separated layer sizes into a list of ints.\"\"\"\n",
        "    s = input(prompt).strip()\n",
        "    sizes = [int(x) for x in s.split(',') if x.strip()]\n",
        "    if len(sizes) < 2:\n",
        "        raise ValueError(\"Please specify at least two layer sizes (input and output).\")\n",
        "    return sizes\n",
        "\n",
        "\n",
        "# === 1. Interactive parameter input ===\n",
        "print(\"=== Differential Privacy Split Learning MNIST ===\")\n",
        "dp_type = input(\"Select DP type ('fixed' or 'adaptive'): \").strip().lower()\n",
        "if dp_type not in ('fixed', 'adaptive'):\n",
        "    raise ValueError(\"Invalid DP type; choose 'fixed' or 'adaptive'.\")\n",
        "\n",
        "client_layers = parse_layer_sizes(\n",
        "    \"Enter client‐side layer sizes (e.g. 784,256,128): \")\n",
        "server_layers = parse_layer_sizes(\n",
        "    \"Enter server‐side layer sizes (e.g. 128,64,10): \")\n",
        "\n",
        "batch_size      = int(input(\"Batch size [64]: \") or 64)\n",
        "epochs          = int(input(\"Epochs [10]: \") or 10)\n",
        "learning_rate   = float(input(\"Learning rate [0.01]: \") or 0.01)\n",
        "\n",
        "if dp_type == 'fixed':\n",
        "    clip_norm       = float(input(\"Clip norm [1.0]: \") or 1.0)\n",
        "    noise_multiplier= float(input(\"Noise multiplier σ [0.1]: \") or 0.1)\n",
        "else:\n",
        "    clip_norm        = float(input(\"Initial clip norm [1.0]: \") or 1.0)\n",
        "    noise_multiplier = float(input(\"Initial noise multiplier σ [0.1]: \") or 0.1)\n",
        "    alpha            = float(input(\"Adaptive α [0.2]: \") or 0.2)\n",
        "    tau              = float(input(\"Adaptive τ [0.1]: \") or 0.1)\n",
        "    beta             = float(input(\"Adaptive β [0.9]: \") or 0.9)\n",
        "    # We'll track validation improvements to decay noise_multiplier\n",
        "    best_val_loss = float('inf')\n",
        "    val_improve_count = 0\n",
        "\n",
        "# === 2. Define split models ===\n",
        "class ClientNet(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super().__init__()\n",
        "        modules = []\n",
        "        for i in range(len(layers)-1):\n",
        "            modules.append(nn.Linear(layers[i], layers[i+1]))\n",
        "            # no activation on last client layer\n",
        "            if i < len(layers)-2:\n",
        "                modules.append(nn.ReLU())\n",
        "        self.net = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class ServerNet(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super().__init__()\n",
        "        modules = []\n",
        "        for i in range(len(layers)-1):\n",
        "            modules.append(nn.Linear(layers[i], layers[i+1]))\n",
        "            # no ReLU on last server layer\n",
        "            if i < len(layers)-2:\n",
        "                modules.append(nn.ReLU())\n",
        "        self.net = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# === 3. Data loaders ===\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "train_ds = datasets.MNIST('./data', train=True,  download=True, transform=transform)\n",
        "test_ds  = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# === 4. Setup ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "client = ClientNet(client_layers).to(device)\n",
        "server = ServerNet(server_layers).to(device)\n",
        "\n",
        "opt_client = optim.SGD(client.parameters(), lr=learning_rate)\n",
        "opt_server = optim.SGD(server.parameters(), lr=learning_rate)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# === 5. Training loop with server-side DP noise ===\n",
        "start_time = datetime.now()\n",
        "print(f\"Starting training at {start_time.isoformat()} on {device}\")\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    client.train(); server.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Forward pass: client ➔ server\n",
        "        activation = client(data)\n",
        "        output     = server(activation)\n",
        "        loss       = criterion(output, target)\n",
        "\n",
        "        # Zero grads\n",
        "        opt_client.zero_grad()\n",
        "        opt_server.zero_grad()\n",
        "\n",
        "        # Backpropagate through both parts\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        # === DP noise on server-side gradients only ===\n",
        "        # Compute total L2 norm of server grads\n",
        "        total_norm_sq = 0.0\n",
        "        for p in server.parameters():\n",
        "            if p.grad is not None:\n",
        "                total_norm_sq += p.grad.data.norm(2).item() ** 2\n",
        "        total_norm = total_norm_sq ** 0.5\n",
        "\n",
        "        # Determine clip threshold\n",
        "        current_clip = clip_norm\n",
        "\n",
        "        # Clip server gradients if needed\n",
        "        clip_coef = current_clip / (total_norm + 1e-6)\n",
        "        if clip_coef < 1.0:\n",
        "            for p in server.parameters():\n",
        "                if p.grad is not None:\n",
        "                    p.grad.data.mul_(clip_coef)\n",
        "\n",
        "        # Add Gaussian noise\n",
        "        for p in server.parameters():\n",
        "            if p.grad is not None:\n",
        "                noise = torch.randn_like(p.grad) * noise_multiplier * current_clip\n",
        "                p.grad.data.add_(noise)\n",
        "\n",
        "        # Adaptive update of clip_norm and noise_multiplier\n",
        "        if dp_type == 'adaptive':\n",
        "            # Update clip_norm via exponential moving average\n",
        "            new_clip = tau * (alpha * total_norm) + (1 - tau) * clip_norm\n",
        "            clip_norm = float(new_clip)\n",
        "            # We'll do validation check at epoch end to decay noise_multiplier\n",
        "\n",
        "        # Optimizer steps\n",
        "        opt_server.step()\n",
        "        opt_client.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # Validation for adaptive DP\n",
        "    val_loss = avg_loss\n",
        "    if dp_type == 'adaptive':\n",
        "        if val_loss < best_val_loss:\n",
        "            val_improve_count += 1\n",
        "            best_val_loss = val_loss\n",
        "            if val_improve_count >= 3:\n",
        "                noise_multiplier *= beta\n",
        "                val_improve_count = 0\n",
        "                print(f\"[Epoch {epoch}] Validation improved 3x, decaying noise_multiplier to {noise_multiplier:.4f}\")\n",
        "        else:\n",
        "            val_improve_count = 0\n",
        "\n",
        "    print(f\"Epoch {epoch}/{epochs} — Train Loss: {avg_loss:.4f} \"\n",
        "          f\"— Clip Norm: {clip_norm:.4f} — Noise σ: {noise_multiplier:.4f}\")\n",
        "\n",
        "end_time = datetime.now()\n",
        "duration = (end_time - start_time).total_seconds()\n",
        "print(f\"Finished at {end_time.isoformat()}, duration: {duration:.2f}s\")\n",
        "\n",
        "# === 6. Logging experiment details ===\n",
        "log_entry = {\n",
        "    \"start_time\":        start_time.isoformat(),\n",
        "    \"end_time\":          end_time.isoformat(),\n",
        "    \"duration_seconds\":  duration,\n",
        "    \"dataset\":           \"MNIST\",\n",
        "    \"dp_type\":           dp_type,\n",
        "    \"client_layers\":     client_layers,\n",
        "    \"server_layers\":     server_layers,\n",
        "    \"clip_norm\":         clip_norm,\n",
        "    \"noise_multiplier\":  noise_multiplier,\n",
        "    \"epochs\":            epochs,\n",
        "    \"batch_size\":        batch_size,\n",
        "    \"learning_rate\":     learning_rate\n",
        "}\n",
        "with open(\"experiment_log.jsonl\", \"a\") as lf:\n",
        "    lf.write(json.dumps(log_entry) + \"\\n\")\n",
        "\n",
        "print(\"Experiment parameters and results written to experiment_log.jsonl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IfjxZLx0WC3",
        "outputId": "f554b147-36fd-423b-f943-745f1774ab72"
      },
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Differential Privacy Split Learning MNIST ===\n",
            "Select DP type ('fixed' or 'adaptive'): fixed\n",
            "Enter client‐side layer sizes (e.g. 784,256,128): 784,128,128,32\n",
            "Enter server‐side layer sizes (e.g. 128,64,10): 32,32,10\n",
            "Batch size [64]: 64\n",
            "Epochs [10]: 20\n",
            "Learning rate [0.01]: 0.001\n",
            "Clip norm [1.0]: 1.0\n",
            "Noise multiplier σ [0.1]: 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.4MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 493kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.50MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.59MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training at 2025-04-25T12:29:19.163011 on cuda\n",
            "Epoch 1/20 — Train Loss: 2.2953 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 2/20 — Train Loss: 2.2661 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 3/20 — Train Loss: 2.2078 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 4/20 — Train Loss: 2.0706 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 5/20 — Train Loss: 1.8212 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 6/20 — Train Loss: 1.4920 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 7/20 — Train Loss: 1.1136 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 8/20 — Train Loss: 0.8155 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 9/20 — Train Loss: 0.6503 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 10/20 — Train Loss: 0.5625 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 11/20 — Train Loss: 0.5093 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 12/20 — Train Loss: 0.4730 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 13/20 — Train Loss: 0.4448 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 14/20 — Train Loss: 0.4221 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 15/20 — Train Loss: 0.4029 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 16/20 — Train Loss: 0.3870 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 17/20 — Train Loss: 0.3726 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 18/20 — Train Loss: 0.3602 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 19/20 — Train Loss: 0.3486 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Epoch 20/20 — Train Loss: 0.3382 — Clip Norm: 1.0000 — Noise σ: 0.1000\n",
            "Finished at 2025-04-25T12:33:39.347878, duration: 260.18s\n",
            "Experiment parameters and results written to experiment_log.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Split‐Learning DP MNIST Training Script\n",
        "- Client‐side and Server‐side architectures are defined via CLI flags.\n",
        "- Noise added only on the server side gradients (fixed or adaptive DP).\n",
        "- Logs all experiment parameters and outcomes to a JSONL log file.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def parse_layer_sizes(s: str):\n",
        "    \"\"\"Parse comma‐separated layer sizes into a list of ints.\"\"\"\n",
        "    sizes = [int(x) for x in s.split(',') if x.strip()]\n",
        "    if len(sizes) < 2:\n",
        "        raise argparse.ArgumentTypeError(\n",
        "            \"Must specify at least two layer sizes, e.g. 784,256,10\"\n",
        "        )\n",
        "    return sizes\n",
        "\n",
        "\n",
        "# === 1. Parse CLI arguments ===\n",
        "parser = argparse.ArgumentParser(\n",
        "    description=\"Split‐Learning DP MNIST Training\")\n",
        "parser.add_argument(\n",
        "    \"--dp-type\", choices=(\"fixed\", \"adaptive\"), required=True,\n",
        "    help=\"Differential privacy mode: 'fixed' or 'adaptive'\")\n",
        "parser.add_argument(\n",
        "    \"--client-layers\", type=parse_layer_sizes, required=True,\n",
        "    help=\"Client‐side layer sizes, e.g. '784,256,128'\")\n",
        "parser.add_argument(\n",
        "    \"--server-layers\", type=parse_layer_sizes, required=True,\n",
        "    help=\"Server‐side layer sizes, e.g. '128,64,10'\")\n",
        "parser.add_argument(\n",
        "    \"--batch-size\", type=int, default=64, help=\"Batch size (default: 64)\")\n",
        "parser.add_argument(\n",
        "    \"--epochs\", type=int, default=10, help=\"Number of epochs (default: 10)\")\n",
        "parser.add_argument(\n",
        "    \"--learning-rate\", type=float, default=0.01,\n",
        "    help=\"SGD learning rate (default: 0.01)\")\n",
        "# DP parameters\n",
        "parser.add_argument(\n",
        "    \"--clip-norm\", type=float, default=1.0,\n",
        "    help=\"Clipping norm (default: 1.0)\")\n",
        "parser.add_argument(\n",
        "    \"--noise-multiplier\", type=float, default=0.1,\n",
        "    help=\"Gaussian noise multiplier σ (default: 0.1)\")\n",
        "# Adaptive‐DP only\n",
        "parser.add_argument(\n",
        "    \"--alpha\", type=float, default=0.2,\n",
        "    help=\"Adaptive DP α factor (default: 0.2)\")\n",
        "parser.add_argument(\n",
        "    \"--tau\", type=float, default=0.1,\n",
        "    help=\"Adaptive DP τ smoothing (default: 0.1)\")\n",
        "parser.add_argument(\n",
        "    \"--beta\", type=float, default=0.9,\n",
        "    help=\"Adaptive DP β decay (default: 0.9)\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "dp_type         = args.dp_type\n",
        "client_layers   = args.client_layers\n",
        "server_layers   = args.server_layers\n",
        "batch_size      = args.batch_size\n",
        "epochs          = args.epochs\n",
        "learning_rate   = args.learning_rate\n",
        "clip_norm       = args.clip_norm\n",
        "noise_multiplier= args.noise_multiplier\n",
        "alpha           = args.alpha\n",
        "tau             = args.tau\n",
        "beta            = args.beta\n",
        "\n",
        "if dp_type == \"adaptive\":\n",
        "    best_val_loss     = float('inf')\n",
        "    val_improve_count = 0\n",
        "\n",
        "# === 2. Define split models ===\n",
        "class ClientNet(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super().__init__()\n",
        "        modules = []\n",
        "        for i in range(len(layers)-1):\n",
        "            modules.append(nn.Linear(layers[i], layers[i+1]))\n",
        "            if i < len(layers)-2:\n",
        "                modules.append(nn.ReLU())\n",
        "        self.net = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x.view(x.size(0), -1))\n",
        "\n",
        "\n",
        "class ServerNet(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super().__init__()\n",
        "        modules = []\n",
        "        for i in range(len(layers)-1):\n",
        "            modules.append(nn.Linear(layers[i], layers[i+1]))\n",
        "            if i < len(layers)-2:\n",
        "                modules.append(nn.ReLU())\n",
        "        self.net = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# === 3. Data loaders ===\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "train_ds = datasets.MNIST('./data', train=True,  download=True, transform=transform)\n",
        "test_ds  = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# === 4. Setup ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "client = ClientNet(client_layers).to(device)\n",
        "server = ServerNet(server_layers).to(device)\n",
        "\n",
        "opt_client = optim.SGD(client.parameters(), lr=learning_rate)\n",
        "opt_server = optim.SGD(server.parameters(), lr=learning_rate)\n",
        "criterion  = nn.CrossEntropyLoss()\n",
        "\n",
        "# === 5. Training loop with server‐side DP noise ===\n",
        "start_time = datetime.now()\n",
        "print(f\"[{start_time.isoformat()}] Starting {dp_type}-DP split training on {device}\")\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    client.train(); server.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Forward: client → server\n",
        "        activation = client(data)\n",
        "        output     = server(activation)\n",
        "        loss       = criterion(output, target)\n",
        "\n",
        "        opt_client.zero_grad()\n",
        "        opt_server.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        # --- server‐side DP ---\n",
        "        # 1. compute total grad norm\n",
        "        norm_sq = 0.0\n",
        "        for p in server.parameters():\n",
        "            if p.grad is not None:\n",
        "                norm_sq += p.grad.data.norm(2).item()**2\n",
        "        total_norm = norm_sq**0.5\n",
        "\n",
        "        # 2. clip\n",
        "        clip_coef = clip_norm / (total_norm + 1e-6)\n",
        "        if clip_coef < 1.0:\n",
        "            for p in server.parameters():\n",
        "                if p.grad is not None:\n",
        "                    p.grad.data.mul_(clip_coef)\n",
        "\n",
        "        # 3. add Gaussian noise\n",
        "        for p in server.parameters():\n",
        "            if p.grad is not None:\n",
        "                noise = torch.randn_like(p.grad) * noise_multiplier * clip_norm\n",
        "                p.grad.data.add_(noise)\n",
        "\n",
        "        # 4. adaptive update\n",
        "        if dp_type == \"adaptive\":\n",
        "            new_clip = tau * (alpha * total_norm) + (1 - tau) * clip_norm\n",
        "            clip_norm = float(new_clip)\n",
        "\n",
        "        opt_server.step()\n",
        "        opt_client.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # adaptive DP validation‐based decay\n",
        "    if dp_type == \"adaptive\":\n",
        "        if avg_loss < best_val_loss:\n",
        "            val_improve_count += 1\n",
        "            best_val_loss = avg_loss\n",
        "            if val_improve_count >= 3:\n",
        "                noise_multiplier *= beta\n",
        "                val_improve_count = 0\n",
        "                print(f\"  [Epoch {epoch}] Decayed σ → {noise_multiplier:.4f}\")\n",
        "        else:\n",
        "            val_improve_count = 0\n",
        "\n",
        "    print(f\"Epoch {epoch}/{epochs} — Loss: {avg_loss:.4f} \"\n",
        "          f\"— Clip: {clip_norm:.4f} — σ: {noise_multiplier:.4f}\")\n",
        "\n",
        "end_time = datetime.now()\n",
        "duration = (end_time - start_time).total_seconds()\n",
        "print(f\"[{end_time.isoformat()}] Finished; duration {duration:.2f}s\")\n",
        "\n",
        "# === 6. Log results ===\n",
        "log = {\n",
        "    \"start_time\":        start_time.isoformat(),\n",
        "    \"end_time\":          end_time.isoformat(),\n",
        "    \"duration_s\":        duration,\n",
        "    \"dataset\":           \"MNIST\",\n",
        "    \"dp_type\":           dp_type,\n",
        "    \"client_layers\":     client_layers,\n",
        "    \"server_layers\":     server_layers,\n",
        "    \"clip_norm\":         clip_norm,\n",
        "    \"noise_multiplier\":  noise_multiplier,\n",
        "    \"epochs\":            epochs,\n",
        "    \"batch_size\":        batch_size,\n",
        "    \"learning_rate\":     learning_rate,\n",
        "    \"alpha\":             alpha if dp_type==\"adaptive\" else None,\n",
        "    \"tau\":               tau   if dp_type==\"adaptive\" else None,\n",
        "    \"beta\":              beta  if dp_type==\"adaptive\" else None,\n",
        "}\n",
        "\n",
        "with open(\"experiment_log.jsonl\", \"a\") as lf:\n",
        "    lf.write(json.dumps(log) + \"\\n\")\n",
        "\n",
        "print(\"Logged experiment to experiment_log.jsonl\")\n",
        "\n",
        "\"\"\"\n",
        "python dp_split_mnist.py \\\n",
        "  --dp-type adaptive \\\n",
        "  --client-layers 784,256,128 \\\n",
        "  --server-layers 128,64,10 \\\n",
        "  --batch-size 64 \\\n",
        "  --epochs 20 \\\n",
        "  --learning-rate 0.01 \\\n",
        "  --clip-norm 1.0 \\\n",
        "  --noise-multiplier 0.1 \\\n",
        "  --alpha 0.2 \\\n",
        "  --tau 0.1 \\\n",
        "  --beta 0.9\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-CjlZM563YfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o3aDifAx3ZXY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}